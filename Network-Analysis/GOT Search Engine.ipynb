{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7147881a",
   "metadata": {},
   "source": [
    "# Search Engine des alias d'un personnage de Game of Thrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c71e4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import jupyter_black\n",
    "import os\n",
    "jupyter_black.load(lab=False)\n",
    "sns.set()\n",
    "\n",
    "SAVE_PATH = (\n",
    "    \"/home/samy/csv_pickle_parquet/\"  # Le directory ou se trouve les documents d'études\n",
    ")\n",
    "\n",
    "HTML_PATH = SAVE_PATH + \"html_got/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8178fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWOIAF_LIST_URL = \"https://awoiaf.westeros.org/index.php/List_of_characters\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.118 Safari/537.36\"\n",
    "}\n",
    "\n",
    "r = requests.get(AWOIAF_LIST_URL, headers=HEADERS)\n",
    "\n",
    "soup = BeautifulSoup(r.text)\n",
    "items = soup.find_all(\"ul\")[9:35]\n",
    "li_list = []\n",
    "for ul in items:\n",
    "    temp = ul.find_all(\"li\")\n",
    "    for elem in temp:\n",
    "        li_list.append(elem.find(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f765f06",
   "metadata": {},
   "source": [
    "### Création d'un dictionnaire avec tout les liens de téléchargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79093600",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_url_dict = {}\n",
    "for item in li_list:\n",
    "    title = item.get(\"title\")\n",
    "    if title:\n",
    "        href = item.get(\"href\")\n",
    "        characters_url_dict[title] = \"https://awoiaf.westeros.org\" + href"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ba6a0",
   "metadata": {},
   "source": [
    "### Téléchargement des données et transformation en csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_wiki = pd.DataFrame(columns=[\"character\", \"raw_data\"])\n",
    "\n",
    "\n",
    "for chara in tqdm(characters_url_dict.keys()):\n",
    "    fetched_html = requests.get(characters_url_dict[chara], headers=HEADERS).text\n",
    "    raw_data_wiki.loc[len(raw_data_wiki), :] = [chara, fetched_html]\n",
    "\n",
    "\n",
    "raw_data_wiki.to_csv(SAVE_PATH + \"awoiaf_raw_html_v2.csv\", sep=\",\", index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2e7b0",
   "metadata": {},
   "source": [
    "### Obtention de la page html pour un personnage en particulier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e87a3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "\n",
    "FILENAME = SAVE_PATH + \"awoiaf_raw_html_v2.csv\"\n",
    "ENCODING = \"utf-8\"\n",
    "\n",
    "# Deux solutions pour lire des données : import via pandas ou streaming\n",
    "\n",
    "\n",
    "def get_html_streaming(\n",
    "    character,\n",
    "):  # Cette solution est intéressante si la volumétrie de données est très importante (temps d'exec au pire des cas : 3.76s)\n",
    "    with codecs.open(FILENAME, \"r\", ENCODING) as fp:\n",
    "        reader = csv.reader(fp)\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if row[0] == character:\n",
    "                return row[1]\n",
    "        raise Exception(\"Character not found\")\n",
    "\n",
    "\n",
    "def get_html_pandas(\n",
    "    character,\n",
    "):  # Solution naive mais la plus efficace dans le cas présent (temps d'exec moyen : 1.72s)\n",
    "    data = pd.read_csv(FILENAME)\n",
    "    liste = data[data[\"character\"] == character].raw_data.tolist()\n",
    "    if liste:\n",
    "        return liste[0]\n",
    "    else:\n",
    "        raise Exception(\"Character not found\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f4a06",
   "metadata": {},
   "source": [
    "### Obtention des alias d'un personnage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10b5f91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 ms, sys: 0 ns, total: 2.56 ms\n",
      "Wall time: 2.57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BOOKS_TITLE = {\n",
    "    \"A Game of Thrones\": \"GOT1.txt\",\n",
    "    \"A Clash of Kings\": \"GOT2.txt\",\n",
    "    \"A Storm of Swords\": \"GOT3.txt\",\n",
    "    \"A Feast for Crows\": \"GOT4.txt\",\n",
    "    \"A Dance with Dragons\": \"GOT5.txt\",\n",
    "}\n",
    "\n",
    "CHAR_URLS = []\n",
    "for item in li_list:\n",
    "    title = item.get(\"title\")\n",
    "    if title:\n",
    "        href = item.get(\"href\")\n",
    "        CHAR_URLS.append(href)\n",
    "\n",
    "\n",
    "def get_aliases(soup):\n",
    "    aliases_list = []\n",
    "    if soup.find(\"table\", class_=\"infobox\"):\n",
    "        aliases_html_th = soup.find(\"table\", class_=\"infobox\").find(\"th\", text=\"Aliases\")\n",
    "        if aliases_html_th:\n",
    "            aliases_html_td = aliases_html_th.find_next(\"td\").find_all(\"li\")\n",
    "            for elmt in aliases_html_td:\n",
    "                name = elmt.text\n",
    "                name = (\n",
    "                    \"\".join([i for i in name if not i.isdigit()])\n",
    "                    .replace(\"[\", \"\")\n",
    "                    .replace(\"]\", \"\")\n",
    "                )\n",
    "                aliases_list.append(name)\n",
    "    return aliases_list\n",
    "\n",
    "\n",
    "def get_alias(soup):\n",
    "    name = []\n",
    "    if soup.find(\"table\", class_=\"infobox\"):\n",
    "        alias_html_th = soup.find(\"table\", class_=\"infobox\").find(\"th\", text=\"Alias\")\n",
    "        if alias_html_th:\n",
    "            alias = alias_html_th.find_next(\"td\").text\n",
    "            alias = (\n",
    "                \"\".join([i for i in alias if not i.isdigit()])\n",
    "                .replace(\"[\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                .strip()\n",
    "            )\n",
    "            name.append(alias)\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_title_name(soup):\n",
    "    title_html_h1 = soup.find(\"h1\").text\n",
    "    return title_html_h1\n",
    "\n",
    "\n",
    "def get_infobox_name(soup):\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if infobox:\n",
    "        sub_table = infobox.find(\"table\")\n",
    "        if sub_table:\n",
    "            infobox_name = sub_table.find_all(\"td\")[1]\n",
    "            span = infobox_name.find(\"span\")\n",
    "            if span:\n",
    "                span.decompose()\n",
    "            return infobox_name.text\n",
    "        else:\n",
    "            return infobox.find_next(\"th\").text\n",
    "\n",
    "\n",
    "def get_text_length(soup):\n",
    "    text_list = soup.find_all(\"p\")\n",
    "    text = \"\".join(i.text for i in text_list)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def get_books(soup):\n",
    "    book_list = []\n",
    "    if soup.find(\"table\", class_=\"infobox\"):\n",
    "        books_html_th = soup.find(\"table\", class_=\"infobox\").find(\"th\", text=\"Books\")\n",
    "        if books_html_th:\n",
    "            books_html_td = books_html_th.find_next(\"td\").find_all(\"li\")\n",
    "            for elmt in books_html_td:\n",
    "                name = elmt.text\n",
    "                book_list.append(name)\n",
    "    return book_list\n",
    "\n",
    "\n",
    "def get_book(soup):\n",
    "    name = []\n",
    "    if soup.find(\"table\", class_=\"infobox\"):\n",
    "        book_html_th = soup.find(\"table\", class_=\"infobox\").find(\"th\", text=\"Book\")\n",
    "        if book_html_th:\n",
    "            book = book_html_th.find_next(\"td\").text\n",
    "            book = (\n",
    "                \"\".join([i for i in book if not i.isdigit()])\n",
    "                .replace(\"[\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                .strip()\n",
    "            )\n",
    "            name.append(book)\n",
    "    return name\n",
    "\n",
    "def separate_book_appearance(input_book_list):\n",
    "    output_dict = {}\n",
    "    output_book_list,output_appearance_list = [],[]\n",
    "    \n",
    "    for book in input_book_list:\n",
    "        if '(' in book:\n",
    "            string_list = book.split('(')\n",
    "            string_list[1] = string_list[1].replace(\")\",\"\")\n",
    "            output_dict[string_list[0].strip()] = string_list[1]\n",
    "        else:\n",
    "            output_dict[book] = \"Undefined\"\n",
    "    for key in output_dict:\n",
    "        if key in BOOKS_TITLE.keys():\n",
    "            \n",
    "            output_book_list.append(BOOKS_TITLE[key])\n",
    "            output_appearance_list.append(output_dict[key])\n",
    "            \n",
    "    return output_book_list,output_appearance_list\n",
    "separate_book_appearance(stark)\n",
    "\n",
    "\n",
    "def get_page_rank(soup):\n",
    "    rank = 0\n",
    "    link_list = soup.find_all(\"a\")\n",
    "    for link in link_list:\n",
    "        href = link.get(\"href\")\n",
    "        if href in CHAR_URLS:\n",
    "            rank += 1\n",
    "    return rank\n",
    "\n",
    "\n",
    "def get_full_page(soup):\n",
    "    text = soup.select(\"div#mw-content-text\")[0].text\n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    clean_text = text\n",
    "    clean_text = re.sub(r\"{^\\w\\s}\",\"\",clean_text)\n",
    "    string_encode = clean_text.encode(\"ascii\",\"ignore\")\n",
    "    clean_text = string_encode.decode()\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def get_total_information(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    if soup:\n",
    "        book_list = get_books(soup) + get_book(soup)\n",
    "        book_number,appearances = separate_book_appearance(book_list)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"name_title\": get_title_name(soup),\n",
    "            \"name_infobox\": get_infobox_name(soup),\n",
    "            \"aliases\": get_aliases(soup) + get_alias(soup),\n",
    "            \"page_rank\": np.nan,\n",
    "            \"text_length\": get_text_length(soup),\n",
    "            \"books\": book_number,\n",
    "            \"nature_of_appearance\":appearances,\n",
    "            \"html\":html\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"name_title\": np.nan,\n",
    "            \"name_infobox\": np.nan,\n",
    "            \"aliases\": np.nan,\n",
    "            \"page_rank\": np.nan,\n",
    "            \"text_length\": np.nan,\n",
    "            \"books\": np.nan,\n",
    "            \"nature_of_appearance\":np.nan,\n",
    "            \"html\":html\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbd1b9",
   "metadata": {},
   "source": [
    "###### Transfert de toutes les infos vers un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fe64754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"name_title\",\n",
    "            \"name_infobox\",\n",
    "            \"aliases\",\n",
    "            \"page_rank\",\n",
    "            \"text_length\",\n",
    "            \"books\",\n",
    "            \"nature_of_appearance\",\n",
    "            \"html\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "071267f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3669/3669 [01:21<00:00, 45.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for dirpath,dirnames,filenames in os.walk(HTML_PATH):\n",
    "    for filename in tqdm(filenames):\n",
    "        if filename.endswith(\".html\"):\n",
    "            filepath = os.path.join(dirpath,filename)\n",
    "            with open(filepath,\"r\") as fp:\n",
    "                html = fp.read()\n",
    "                data.loc[len(data),:] = get_total_information(html)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f1b3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(SAVE_PATH + \"got_characters_information.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
